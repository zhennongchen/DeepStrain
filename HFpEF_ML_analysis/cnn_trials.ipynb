{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ema_pytorch import EMA\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "from scipy.ndimage import zoom\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import nibabel as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import DeepStrain.functions_collection as ff\n",
    "import DeepStrain.Data_processing as dp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_path = '/mnt/camca_NAS/HFpEF/data/HFpEF_data/Patient_list/Important_HFpEF_Patient_list_unique_patient_w_readmission_finalized.xlsx' \n",
    "patient_list = pd.read_excel(spreadsheet_path)[0:50]\n",
    "dl_seg_path = '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF'\n",
    "\n",
    "index_list = []\n",
    "for i in range(0, len(patient_list)):\n",
    "    patient_id = patient_list['OurID'][i]; patient_id = ff.XX_to_ID_00XX(patient_id)\n",
    "    if os.path.isdir(os.path.join(dl_seg_path, patient_id)):\n",
    "        index_list.append(i)\n",
    "patient_list = patient_list.iloc[index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_0085\n",
      "['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "(15, 64, 64, 15)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(patient_list)):\n",
    "    patient_id = patient_list['OurID'].iloc[i]; patient_id = ff.XX_to_ID_00XX(patient_id)\n",
    "    # load data\n",
    "    print(patient_id)\n",
    "    pred_seg_files = ff.sort_timeframe(ff.find_all_target_files(['pred_seg*'], os.path.join(dl_seg_path, patient_id, 'epoch-81')),2,'_','.')\n",
    "    print(pred_seg_files)\n",
    "\n",
    "    # load one for dimension\n",
    "    a = nb.load(pred_seg_files[0]).get_fdata()\n",
    "\n",
    "    # load 4D\n",
    "    pred_seg = np.zeros([len(pred_seg_files), a.shape[0], a.shape[1], a.shape[2]])\n",
    "    for j in range(0, len(pred_seg_files)):\n",
    "        pred_seg[j] = nb.load(pred_seg_files[j]).get_fdata()\n",
    "    pred_seg = np.transpose(pred_seg, (3,1,2,0))\n",
    "\n",
    "    # pad slice\n",
    "    pred_seg_pad = np.zeros([a.shape[-1], a.shape[0], a.shape[1], 15])\n",
    "    if pred_seg.shape[-1] < 15:\n",
    "        pad = 15 - pred_seg.shape[-1]\n",
    "        pad_before = pad//2\n",
    "        pad_after = pad - pad_before\n",
    "        for k in range(0, pred_seg.shape[0]):\n",
    "            a = pred_seg[k,...]\n",
    "            pred_seg_pad[k] = np.pad(a, ((0,0),(0,0),(pad_before,pad_after)))\n",
    "  \n",
    "    # pad x y\n",
    "    pred_seg_final = np.zeros([15, 128,128, 15])\n",
    "    for j in range(0, pred_seg_pad.shape[0]):\n",
    "        pred_seg_final[j,...],_,_ = dp.center_crop(pred_seg_pad[j,...], pred_seg_pad[j,...], [128,128,15], according_to_which_class=[1])\n",
    "    \n",
    "    # downsample x and y by 2\n",
    "    pred_seg_ds = np.zeros([15, 64,64, 15])\n",
    "    for j in range(0, pred_seg_final.shape[0]):\n",
    "        pred_seg_ds[j,...] = zoom(pred_seg_final[j,...], (0.5,0.5,1), order=0)\n",
    "    print(pred_seg_ds.shape)\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x900 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot seg_ed and seg_es\n",
    "fig, ax = plt.subplots(3,5, figsize = (15,9))\n",
    "for i in range(15):\n",
    "    ax[i//5, i%5].imshow(pred_seg_ds[i,:,:,3])\n",
    "    ax[i//5, i%5].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_seg_EDES(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_list,\n",
    "        manual_seg_path,\n",
    "        shuffle = False,\n",
    "        augment = False,\n",
    "        augment_frequency = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patient_list = patient_list\n",
    "        self.manual_seg_path = manual_seg_path\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.augment_frequency = augment_frequency\n",
    "        self.num_files = patient_list.shape[0]\n",
    "\n",
    "        self.index_array = self.generate_index_array()\n",
    "       \n",
    "    def generate_index_array(self):\n",
    "        np.random.seed()\n",
    "        index_array = []\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            index_array = np.random.permutation(self.num_files)\n",
    "        else:\n",
    "            index_array = np.arange(self.num_files)\n",
    "        return index_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files \n",
    "    \n",
    "    def load_seg(self, patient_id):\n",
    "        seg_file_ed = os.path.join(self.manual_seg_path, patient_id, 'SAX_ED_seg.nii.gz')\n",
    "        seg_file_es = os.path.join(self.manual_seg_path, patient_id, 'SAX_ES_seg.nii.gz')\n",
    "\n",
    "        seg_ed = nb.load(seg_file_ed).get_fdata(); seg_ed = np.round(seg_ed).astype(int); seg_ed[seg_ed == 1] = 0; seg_ed[seg_ed > 1] = 1\n",
    "        seg_es = nb.load(seg_file_es).get_fdata(); seg_es = np.round(seg_es).astype(int); seg_es[seg_es == 1] = 0; seg_es[seg_es > 1] = 1\n",
    "\n",
    "        # find the slices with content in seg_ed\n",
    "        slice_idx = np.where(np.sum(seg_ed, axis = (0,1))>0)[0]\n",
    "\n",
    "        # only keep these slices\n",
    "        seg_ed = seg_ed[:,:,slice_idx]\n",
    "        seg_es = seg_es[:,:,slice_idx]\n",
    "\n",
    "        # make slice number = 15 by padding zero slices and make sure the non-zero slices are in the middle\n",
    "        if slice_idx.shape[0] < 15:\n",
    "            pad = 15 - slice_idx.shape[0]\n",
    "            pad_before = pad//2\n",
    "            pad_after = pad - pad_before\n",
    "            seg_ed = np.pad(seg_ed, ((0,0),(0,0),(pad_before,pad_after)))\n",
    "            seg_es = np.pad(seg_es, ((0,0),(0,0),(pad_before,pad_after)))\n",
    "        elif slice_idx.shape[0] > 15:\n",
    "            ValueError('the number of slices in seg_ed is larger than 15')\n",
    "        \n",
    "        seg_ed,_,_ = dp.center_crop(seg_ed, seg_ed, [128,128,15],according_to_which_class=[1])\n",
    "        seg_es,_,_ = dp.center_crop(seg_es, seg_es, [128,128,15],according_to_which_class=[1])\n",
    "        \n",
    "        # stack the seg_ed and seg_es\n",
    "        seg_ed = np.expand_dims(seg_ed, axis = -1)\n",
    "        seg_es = np.expand_dims(seg_es, axis = -1)\n",
    "        seg = np.concatenate((seg_ed, seg_es), axis = -1)\n",
    "\n",
    "        return seg\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # print('in this geiitem, self.index_array is: ', self.index_array)\n",
    "        f = self.index_array[index]\n",
    "        row = self.patient_list.iloc[f]\n",
    "        patient_id = row['OurID']; patient_id = ff.XX_to_ID_00XX(patient_id)\n",
    "        # print('index is: ', index, ' now we pick patient: ', patient_id)\n",
    "        \n",
    "        # load the x data\n",
    "        x = self.load_seg(patient_id)\n",
    "        if self.augment == True:\n",
    "            if random.uniform(0,1) < self.augment_frequency:\n",
    "                x_ed, z_rotate_degree = dp.random_rotate(x[:,:,:,0], z_rotate_range = [-10,10], order = 0)\n",
    "                x_ed, x_translate, y_translate  = dp.random_translate(x_ed, translate_range=[-10,10])\n",
    "                x_es, z_rotate_degree = dp.random_rotate(x[:,:,:,1], z_rotate_degree= z_rotate_degree, order = 0)\n",
    "                x_es, x_translate, y_translate  = dp.random_translate(x_es, x_translate = x_translate, y_translate = y_translate)\n",
    "                x = np.stack((x_ed, x_es), axis = -1)\n",
    "        x = np.transpose(x, (3,0,1,2))\n",
    "            \n",
    "        # load the y value\n",
    "        y = np.array([row['label_for_ML']])\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        print('now run on_epoch_end function')\n",
    "        self.index_array = self.generate_index_array()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_seg_full(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_list,\n",
    "        dl_seg_path,\n",
    "        shuffle = False,\n",
    "        augment = False,\n",
    "        augment_frequency = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patient_list = patient_list\n",
    "        self.dl_seg_path = dl_seg_path\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.augment_frequency = augment_frequency\n",
    "        self.num_files = patient_list.shape[0]\n",
    "\n",
    "        self.index_array = self.generate_index_array()\n",
    "       \n",
    "    def generate_index_array(self):\n",
    "        np.random.seed()\n",
    "        index_array = []\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            index_array = np.random.permutation(self.num_files)\n",
    "        else:\n",
    "            index_array = np.arange(self.num_files)\n",
    "        return index_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files \n",
    "    \n",
    "    def load_seg(self, patient_id):\n",
    "        # load data\n",
    "        pred_seg_files = ff.sort_timeframe(ff.find_all_target_files(['pred_seg*'], os.path.join(self.dl_seg_path, patient_id, 'epoch-81')),2,'_','.')\n",
    "        # load one for dimension\n",
    "        # print('pred_seg_files: ', pred_seg_files)\n",
    "        a = nb.load(pred_seg_files[0]).get_fdata()\n",
    "\n",
    "        # load 4D\n",
    "        pred_seg = np.zeros([len(pred_seg_files), a.shape[0], a.shape[1], a.shape[2]])\n",
    "        for j in range(0, len(pred_seg_files)):\n",
    "            pred_seg[j] = nb.load(pred_seg_files[j]).get_fdata()\n",
    "        pred_seg = np.transpose(pred_seg, (3,1,2,0))\n",
    "\n",
    "        # pad slice\n",
    "        pred_seg_pad = np.zeros([a.shape[-1], a.shape[0], a.shape[1], 15])\n",
    "        if pred_seg.shape[-1] < 15:\n",
    "            pad = 15 - pred_seg.shape[-1]\n",
    "            pad_before = pad//2\n",
    "            pad_after = pad - pad_before\n",
    "            for k in range(0, pred_seg.shape[0]):\n",
    "                a = pred_seg[k,...]\n",
    "                pred_seg_pad[k] = np.pad(a, ((0,0),(0,0),(pad_before,pad_after)))\n",
    "    \n",
    "        # pad x y\n",
    "        pred_seg_final = np.zeros([15, 128,128, 15])\n",
    "        for j in range(0, pred_seg_pad.shape[0]):\n",
    "            pred_seg_final[j,...],_,_ = dp.center_crop(pred_seg_pad[j,...], pred_seg_pad[j,...], [128,128,15], according_to_which_class=[1])\n",
    "        \n",
    "        # downsample x and y by 2\n",
    "        # pred_seg_ds = np.zeros([15, 64,64, 15])\n",
    "        # for j in range(0, pred_seg_final.shape[0]):\n",
    "        #     pred_seg_ds[j,...] = zoom(pred_seg_final[j,...], (0.5,0.5,1), order=0)\n",
    "\n",
    "        return pred_seg_final\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # print('in this geiitem, self.index_array is: ', self.index_array)\n",
    "        f = self.index_array[index]\n",
    "        row = self.patient_list.iloc[f]\n",
    "        patient_id = row['OurID']; patient_id = ff.XX_to_ID_00XX(patient_id)\n",
    "        # print('index is: ', index, ' now we pick patient: ', patient_id)\n",
    "        \n",
    "        # load the x data\n",
    "        x = self.load_seg(patient_id)\n",
    "        if self.augment == True:\n",
    "            if random.uniform(0,1) < self.augment_frequency:\n",
    "                for volume_n in range(0, x.shape[0]):\n",
    "                    if volume_n == 0:\n",
    "                        x[volume_n,...], z_rotate_degree = dp.random_rotate(x[volume_n,...], z_rotate_range = [-10,10], order = 0)\n",
    "                        x[volume_n,...], x_translate, y_translate  = dp.random_translate(x[volume_n,...], translate_range=[-10,10])\n",
    "                    else:\n",
    "                        x[volume_n,...], z_rotate_degree = dp.random_rotate(x[volume_n,...], z_rotate_degree= z_rotate_degree, order = 0)\n",
    "                        x[volume_n,...], x_translate, y_translate  = dp.random_translate(x[volume_n,...], x_translate = x_translate, y_translate = y_translate)\n",
    "        # print('after augmentation, x shape is: ', x.shape)\n",
    "        # load the y value\n",
    "        y = np.array([row['label_for_ML']])\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        # print('finally, x shape and y shape are: ', x.shape, y.shape, ' y value is : ', y)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        print('now run on_epoch_end function')\n",
    "        self.index_array = self.generate_index_array()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trn = Dataset_seg_full(patient_list, dl_seg_path, shuffle = True, augment = True, augment_frequency = 0.5)\n",
    "a = DataLoader(dataset_trn, batch_size = 1, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index is:  3  now we pick patient:  ID_1354\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1354/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  16  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "            \n",
    "cycle_dl = cycle(a)\n",
    "\n",
    "count = 0\n",
    "for batch in cycle_dl:\n",
    "    data = batch\n",
    "    count += 1\n",
    "\n",
    "    if count == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 128, 128, 15)\n"
     ]
    }
   ],
   "source": [
    "pred_seg_ds = data[0][0].numpy()\n",
    "print(pred_seg_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x900 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot seg_ed and seg_es\n",
    "fig, ax = plt.subplots(3,5, figsize = (15,9))\n",
    "for i in range(15):\n",
    "    ax[i//5, i%5].imshow(pred_seg_ds[3,:,:,i])\n",
    "    ax[i//5, i%5].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        # 3D Convolutional layer to process the H, W, and Z dimensions\n",
    "        self.conv1 = nn.Conv3d(in_channels=self.input_channels, out_channels=32, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.conv2 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(3, 3,3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.conv3 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        self.conv4 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(256)\n",
    "        \n",
    "        # 2D MaxPooling layers (applied on H and W dimensions)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(2, 2, 1))  # MaxPool on H and W only\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8 * 15, 512)  # Flattened size after pooling\n",
    "        self.fc2 = nn.Linear(512, 2)  # Final output layer for 2 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Pass through the second convolutional block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Pass through the third convolutional block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Pass through the fourth convolutional block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        generator_trn,\n",
    "        generator_val,\n",
    "        train_batch_size,\n",
    "        *,\n",
    "        accum_iter = 1, # gradient accumulation steps\n",
    "        train_num_steps = 100, # total training epochs\n",
    "        results_folder = None,\n",
    "        train_lr = 1e-4,\n",
    "        train_lr_decay_every = 1000, \n",
    "        save_models_every = 1,\n",
    "        \n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.95,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        max_grad_norm = 1.,):\n",
    "        super().__init__()\n",
    "\n",
    "        # accelerator\n",
    "        # model\n",
    "        self.model = model \n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches = True,\n",
    "            mixed_precision = 'no'\n",
    "        ) \n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "        self.batch_size = train_batch_size\n",
    "        self.accum_iter = accum_iter\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # dataset and dataloader\n",
    "        self.ds_trn = generator_trn\n",
    "        self.dl_trn = DataLoader(self.ds_trn, batch_size = train_batch_size, shuffle = False, pin_memory = True, num_workers = 0)# cpu_count())\n",
    "        self.dl_trn = self.accelerator.prepare(self.dl_trn)\n",
    "\n",
    "        self.ds_val = generator_val\n",
    "        self.dl_val = DataLoader(self.ds_val, batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0)# cpu_count())\n",
    "        self.dl_val = self.accelerator.prepare(self.dl_val)\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = Adam(model.parameters(), lr = train_lr, betas = adam_betas)\n",
    "        self.scheduler = StepLR(self.opt, step_size = 1, gamma=0.95)\n",
    "        self.train_lr_decay_every = train_lr_decay_every\n",
    "        self.save_model_every = save_models_every\n",
    "        self.criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema = EMA(model, beta = ema_decay, update_every = ema_update_every)\n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = results_folder\n",
    "    \n",
    "        ff.make_folder([self.results_folder])\n",
    "\n",
    "        # step counter state\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, stepNum):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'decay_steps': self.scheduler.state_dict(), }\n",
    "        \n",
    "        torch.save(data, os.path.join(self.results_folder, 'model-' + str(stepNum) + '.pt'))\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                x,y = batch; x = x.to(self.device); y = y.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(x)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                # Append predictions and true labels\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "                \n",
    "                # Calculate correct predictions\n",
    "                correct += (predicted == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        # Print the results\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f'Sensitivity (Recall): {sensitivity:.4f}')\n",
    "        print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "        return accuracy, sensitivity, specificity\n",
    "\n",
    "    def load_model(self, trained_model_filename):\n",
    "\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(trained_model_filename, map_location=device)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        self.scheduler.load_state_dict(data['decay_steps'])\n",
    "\n",
    "\n",
    "    def train(self, pre_trained_model = None ,start_step = None):\n",
    "        # set manual torch seed equal to 10\n",
    "        torch.manual_seed(10)\n",
    "\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        training_log = []\n",
    "\n",
    "        # load pre-trained\n",
    "        if pre_trained_model is not None:\n",
    "            self.load_model(pre_trained_model)\n",
    "            print('model loaded from ', pre_trained_model)\n",
    "\n",
    "        if start_step is not None:\n",
    "            self.step = start_step\n",
    "\n",
    "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
    "            \n",
    "            while self.step < self.train_num_steps:\n",
    "                print('training epoch: ', self.step + 1)\n",
    "                print('learning rate: ', self.scheduler.get_last_lr()[0])\n",
    "\n",
    "                average_loss = []\n",
    "                count = 1\n",
    "                # load data\n",
    "                for batch in self.dl_trn:\n",
    "                    if count == 1 or count % self.accum_iter == 1 or count == len(self.dl_trn) - 1 or count == len(self.dl_trn):\n",
    "                        self.opt.zero_grad()\n",
    "         \n",
    "                    # load data\n",
    "                    batch_x, batch_y = batch\n",
    "                        \n",
    "                    data_x = batch_x.to(device)\n",
    "                    data_y = batch_y.to(device)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        output = self.model(data_x)\n",
    "                        loss = self.criterion(output, data_y.squeeze(1).long())\n",
    "                        \n",
    "                    # accumulate the gradient, typically used when batch size is small\n",
    "                    if count % self.accum_iter == 0 or count == len(self.dl_trn) - 1 or count == len(self.dl_trn):\n",
    "                        self.accelerator.backward(loss)\n",
    "                        accelerator.wait_for_everyone()\n",
    "                        accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                        self.opt.step()\n",
    "\n",
    "                    count += 1\n",
    "                    average_loss.append(loss.item())\n",
    "                   \n",
    "                average_loss = sum(average_loss) / len(average_loss)\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "\n",
    "                # save the model\n",
    "                if self.step !=0 and self.step % self.save_model_every == 0:\n",
    "                   self.save(self.step)\n",
    "                \n",
    "                if self.step !=0 and self.step % self.train_lr_decay_every == 0:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                self.ema.update()\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    self.ds_trn.on_epoch_end(); self.ds_val.on_epoch_end()\n",
    "                    acc_trn, sen_trn, spe_trn = self.evaluate(self.dl_trn)\n",
    "                    acc_val, sen_val, spe_val = self.evaluate(self.dl_val)\n",
    "\n",
    "                    # save the training log\n",
    "                    training_log.append([self.step,average_loss, acc_trn, sen_trn, spe_trn, acc_val, sen_val, spe_val])\n",
    "                    df = pd.DataFrame(training_log,columns = ['iteration','average_loss','acc_trn','sen_trn','spe_trn','acc_val','sen_val','spe_val'])\n",
    "                    log_folder = os.path.join(os.path.dirname(self.results_folder),'log');ff.make_folder([log_folder])\n",
    "                    df.to_excel(os.path.join(log_folder, 'training_log.xlsx'),index=False)\n",
    "                        \n",
    "                # at the end of each epoch, call on_epoch_end\n",
    "                self.ds_trn.on_epoch_end()\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(patient_list, cross_val_batch_num, val_batch_index, save_split_file = None):\n",
    "    '''X and Y first dimension is the number of cases'''\n",
    "    num_of_cases_in_each_batch = int(patient_list.shape[0] / cross_val_batch_num)\n",
    "\n",
    "    if os.path.isfile(save_split_file):\n",
    "        batches = np.load(save_split_file)\n",
    "    \n",
    "    else:\n",
    "        Y = patient_list['label_for_ML'].values\n",
    "        Y_1_index = np.where(Y == 1)[0]\n",
    "        Y_0_index = np.where(Y == 0)[0]\n",
    "        # split these indexes into 10 groups with similar size\n",
    "        Y_1_index_split = np.array_split(Y_1_index,cross_val_batch_num)\n",
    "\n",
    "        batches = []; start = 0\n",
    "        for b in range(0, cross_val_batch_num):\n",
    "            current_num = Y_1_index_split[b].shape[0]\n",
    "            end = start + (num_of_cases_in_each_batch - current_num)\n",
    "            Y_0_batch = Y_0_index[start:end]\n",
    "  \n",
    "            batch = np.concatenate((Y_0_batch, Y_1_index_split[b]))\n",
    "            batches.append(batch)\n",
    "            start = end\n",
    "        batches = np.asarray(batches); np.save(save_split_file, batches)\n",
    "    val_idx = batches[val_batch_index,:]\n",
    "    train_idx = np.delete(batches, val_batch_index, 0).flatten()\n",
    "    return train_idx, val_idx\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for ED+ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idx:  [ 1  3  5  6  7  0  2  4  9 11  8 10 13 14 15 12 17 18 20 22 16 19 21 23\n",
      " 24 25 27 30 33 34 26 28 29 31 32 37 39 40 41 42]  val_idx:  [35 36 38 43 45 49 44 46 47 48]\n",
      "val labels:  [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "spreadsheet_path = '/mnt/camca_NAS/HFpEF/data/HFpEF_data/Patient_list/Important_HFpEF_Patient_list_unique_patient_w_readmission_finalized.xlsx' \n",
    "patient_list = pd.read_excel(spreadsheet_path)[0:50]\n",
    "manual_seg_path = '/mnt/camca_NAS/HFpEF/data/HFpEF_data/nii_manual_seg'\n",
    "save_path = '/mnt/camca_NAS/Deepstrain/HFpEF_analysis/models/cnn'\n",
    "\n",
    "train_idx, val_idx = split_train_val(patient_list, 5, 4, save_split_file = os.path.join(save_path, 'train_val_split.npy'))\n",
    "print('train_idx: ', train_idx, ' val_idx: ', val_idx)\n",
    "# print the labels in val\n",
    "print('val labels: ', patient_list.iloc[val_idx]['label_for_ML'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_trn = Dataset_seg_EDES(patient_list.iloc[train_idx], manual_seg_path, shuffle = True, augment = True, augment_frequency = 0.5)\n",
    "# dataset_val = Dataset_seg_EDES(patient_list.iloc[val_idx], manual_seg_path, shuffle = False, augment = False)\n",
    "# model = CNNClassifier(input_channels = 2)\n",
    "# trainer = Trainer(model, dataset_trn, dataset_val, train_batch_size = 1, results_folder = os.path.join(save_path, 'models'), accum_iter = 1, train_num_steps = 1000, train_lr = 1e-3, train_lr_decay_every = 1000, save_models_every = 50)\n",
    "# trainer.train()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for full cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idx:  [ 1  3  5  6  0  2  4  9 11  7  8 10 13 12 17 18 20 22 14 15 16 19 21 25\n",
      " 26 29 32 23 24 27 28 30 33 36 37 38]  val_idx:  [31 34 35 39 41 40 42 43 44]\n",
      "val labels:  [0. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "spreadsheet_path = '/mnt/camca_NAS/HFpEF/data/HFpEF_data/Patient_list/Important_HFpEF_Patient_list_unique_patient_w_readmission_finalized.xlsx' \n",
    "patient_list = pd.read_excel(spreadsheet_path)[0:50]\n",
    "dl_seg_path = '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF'\n",
    "\n",
    "index_list = []\n",
    "for i in range(0, len(patient_list)):\n",
    "    patient_id = patient_list['OurID'][i]; patient_id = ff.XX_to_ID_00XX(patient_id)\n",
    "    if os.path.isdir(os.path.join(dl_seg_path, patient_id)):\n",
    "        index_list.append(i)\n",
    "patient_list = patient_list.iloc[index_list]\n",
    "\n",
    "save_path = '/mnt/camca_NAS/Deepstrain/HFpEF_analysis/models/cnn_fullcycle'\n",
    "\n",
    "train_idx, val_idx = split_train_val(patient_list, 5, 4, save_split_file = os.path.join(save_path, 'train_val_split.npy'))\n",
    "print('train_idx: ', train_idx, ' val_idx: ', val_idx)\n",
    "# print the labels in val\n",
    "print('val labels: ', patient_list.iloc[val_idx]['label_for_ML'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  1\n",
      "learning rate:  0.001\n",
      "index is:  0  now we pick patient:  ID_1172\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  1  now we pick patient:  ID_1180\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  2  now we pick patient:  ID_0280\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  3  now we pick patient:  ID_0678\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  4  now we pick patient:  ID_0951\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  5  now we pick patient:  ID_1124\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  6  now we pick patient:  ID_1177\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  7  now we pick patient:  ID_1207\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  8  now we pick patient:  ID_0685\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  9  now we pick patient:  ID_0290\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  10  now we pick patient:  ID_1208\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  11  now we pick patient:  ID_1057\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  12  now we pick patient:  ID_0085\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  13  now we pick patient:  ID_1163\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  14  now we pick patient:  ID_0080\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  15  now we pick patient:  ID_1352\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  16  now we pick patient:  ID_1151\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  17  now we pick patient:  ID_0949\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  18  now we pick patient:  ID_0954\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  19  now we pick patient:  ID_0291\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  20  now we pick patient:  ID_1132\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  21  now we pick patient:  ID_0671\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  22  now we pick patient:  ID_0016\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  23  now we pick patient:  ID_1405\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  24  now we pick patient:  ID_0284\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  25  now we pick patient:  ID_1361\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  26  now we pick patient:  ID_0287\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  27  now we pick patient:  ID_0692\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  28  now we pick patient:  ID_0078\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  29  now we pick patient:  ID_0468\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_9.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_10.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  30  now we pick patient:  ID_0483\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  31  now we pick patient:  ID_1183\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  32  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  33  now we pick patient:  ID_1175\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  34  now we pick patient:  ID_0953\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  35  now we pick patient:  ID_0682\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:17<4:50:47, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now run on_epoch_end function\n",
      "training epoch:  2\n",
      "learning rate:  0.001\n",
      "index is:  0  now we pick patient:  ID_0954\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  1  now we pick patient:  ID_0078\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  2  now we pick patient:  ID_1207\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  3  now we pick patient:  ID_1163\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  4  now we pick patient:  ID_0291\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  5  now we pick patient:  ID_1405\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  6  now we pick patient:  ID_0085\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  7  now we pick patient:  ID_1177\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  8  now we pick patient:  ID_0280\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  9  now we pick patient:  ID_1057\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  10  now we pick patient:  ID_1180\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  11  now we pick patient:  ID_1352\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  12  now we pick patient:  ID_0671\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  13  now we pick patient:  ID_0016\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  14  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  15  now we pick patient:  ID_1208\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  16  now we pick patient:  ID_1124\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  17  now we pick patient:  ID_0953\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  18  now we pick patient:  ID_0287\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  19  now we pick patient:  ID_1172\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  20  now we pick patient:  ID_0685\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  21  now we pick patient:  ID_0949\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  22  now we pick patient:  ID_0483\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  23  now we pick patient:  ID_0290\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  24  now we pick patient:  ID_1132\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  25  now we pick patient:  ID_0692\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  26  now we pick patient:  ID_1151\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  27  now we pick patient:  ID_1175\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  28  now we pick patient:  ID_1361\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  29  now we pick patient:  ID_0080\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  30  now we pick patient:  ID_0951\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  31  now we pick patient:  ID_1183\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  32  now we pick patient:  ID_0284\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  33  now we pick patient:  ID_0682\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  34  now we pick patient:  ID_0678\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  35  now we pick patient:  ID_0468\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_9.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_10.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:36<5:03:03, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now run on_epoch_end function\n",
      "training epoch:  3\n",
      "learning rate:  0.001\n",
      "index is:  0  now we pick patient:  ID_0291\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  1  now we pick patient:  ID_0671\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  2  now we pick patient:  ID_1183\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  3  now we pick patient:  ID_0483\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  4  now we pick patient:  ID_1207\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  5  now we pick patient:  ID_1175\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  6  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  7  now we pick patient:  ID_0080\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  8  now we pick patient:  ID_0085\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  9  now we pick patient:  ID_1057\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  10  now we pick patient:  ID_0954\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  11  now we pick patient:  ID_1151\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  12  now we pick patient:  ID_0953\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  13  now we pick patient:  ID_0678\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  14  now we pick patient:  ID_1177\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  15  now we pick patient:  ID_1163\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  16  now we pick patient:  ID_1405\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  17  now we pick patient:  ID_0685\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  18  now we pick patient:  ID_1132\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  19  now we pick patient:  ID_0682\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  20  now we pick patient:  ID_0280\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  21  now we pick patient:  ID_0951\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  22  now we pick patient:  ID_1361\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  23  now we pick patient:  ID_0468\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_9.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_10.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  24  now we pick patient:  ID_1172\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  25  now we pick patient:  ID_0078\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  26  now we pick patient:  ID_0290\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  27  now we pick patient:  ID_0692\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  28  now we pick patient:  ID_1180\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  29  now we pick patient:  ID_0016\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  30  now we pick patient:  ID_0287\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  31  now we pick patient:  ID_1124\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  32  now we pick patient:  ID_0949\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  33  now we pick patient:  ID_1352\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  34  now we pick patient:  ID_0284\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  35  now we pick patient:  ID_1208\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:53<4:55:12, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now run on_epoch_end function\n",
      "training epoch:  4\n",
      "learning rate:  0.001\n",
      "index is:  0  now we pick patient:  ID_0291\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  1  now we pick patient:  ID_0951\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  2  now we pick patient:  ID_0078\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  3  now we pick patient:  ID_1183\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  4  now we pick patient:  ID_1057\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  5  now we pick patient:  ID_1163\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  6  now we pick patient:  ID_0678\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  7  now we pick patient:  ID_1405\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  8  now we pick patient:  ID_1180\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  9  now we pick patient:  ID_1172\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  10  now we pick patient:  ID_1208\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  11  now we pick patient:  ID_1361\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  12  now we pick patient:  ID_1207\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  13  now we pick patient:  ID_1124\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  14  now we pick patient:  ID_0949\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  15  now we pick patient:  ID_0692\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  16  now we pick patient:  ID_0287\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  17  now we pick patient:  ID_0954\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  18  now we pick patient:  ID_1352\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  19  now we pick patient:  ID_0284\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  20  now we pick patient:  ID_1177\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  21  now we pick patient:  ID_1132\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  22  now we pick patient:  ID_0953\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  23  now we pick patient:  ID_1151\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  24  now we pick patient:  ID_0685\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  25  now we pick patient:  ID_0468\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_9.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_10.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  26  now we pick patient:  ID_0080\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  27  now we pick patient:  ID_0682\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  28  now we pick patient:  ID_0483\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  29  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  30  now we pick patient:  ID_0280\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  31  now we pick patient:  ID_0085\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  32  now we pick patient:  ID_0016\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  33  now we pick patient:  ID_0671\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  34  now we pick patient:  ID_1175\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  35  now we pick patient:  ID_0290\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [01:11<4:56:15, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now run on_epoch_end function\n",
      "training epoch:  5\n",
      "learning rate:  0.001\n",
      "index is:  0  now we pick patient:  ID_0078\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0078/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  1  now we pick patient:  ID_1352\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1352/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  2  now we pick patient:  ID_0685\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0685/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  3  now we pick patient:  ID_0290\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0290/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  4  now we pick patient:  ID_0678\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0678/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  5  now we pick patient:  ID_0284\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0284/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  6  now we pick patient:  ID_1163\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1163/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  7  now we pick patient:  ID_0468\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_9.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0468/epoch-81/pred_seg_10.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  8  now we pick patient:  ID_0287\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0287/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  9  now we pick patient:  ID_0953\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0953/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  10  now we pick patient:  ID_0949\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0949/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  11  now we pick patient:  ID_1181\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1181/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  12  now we pick patient:  ID_1208\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1208/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  13  now we pick patient:  ID_0016\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0016/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  14  now we pick patient:  ID_1207\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1207/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  15  now we pick patient:  ID_0692\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0692/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  16  now we pick patient:  ID_1361\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1361/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  17  now we pick patient:  ID_0951\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0951/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  18  now we pick patient:  ID_0682\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0682/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  19  now we pick patient:  ID_1124\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1124/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  20  now we pick patient:  ID_1177\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1177/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  21  now we pick patient:  ID_1172\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1172/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  22  now we pick patient:  ID_0483\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0483/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  23  now we pick patient:  ID_1057\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1057/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  24  now we pick patient:  ID_1151\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1151/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  25  now we pick patient:  ID_1132\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1132/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  26  now we pick patient:  ID_0671\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0671/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  27  now we pick patient:  ID_0954\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0954/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  28  now we pick patient:  ID_0080\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0080/epoch-81/pred_seg_6.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  29  now we pick patient:  ID_0280\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0280/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  30  now we pick patient:  ID_1180\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1180/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  31  now we pick patient:  ID_0291\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_8.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0291/epoch-81/pred_seg_9.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  32  now we pick patient:  ID_1183\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1183/epoch-81/pred_seg_5.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  33  now we pick patient:  ID_1175\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_7.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1175/epoch-81/pred_seg_8.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([0.])\n",
      "index is:  34  now we pick patient:  ID_0085\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_0085/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n",
      "index is:  35  now we pick patient:  ID_1405\n",
      "pred_seg_files:  ['/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_0.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_1.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_2.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_3.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_4.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_5.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_6.nii.gz'\n",
      " '/mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_2box_text_HF_5shot/predicts_HFpEF/ID_1405/epoch-81/pred_seg_7.nii.gz']\n",
      "finally, x shape and y shape are:  torch.Size([15, 128, 128, 15]) torch.Size([1])  y value is :  tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [01:31<6:18:20, 22.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m CNNClassifier(input_channels\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, dataset_trn, dataset_val, train_batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, results_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_path, \u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m), accum_iter \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, train_num_steps \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m, train_lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m, train_lr_decay_every \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m, save_models_every \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()      \n",
      "\u001b[1;32m/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39m# save the model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_lr_decay_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb Cell 23\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mget_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel),\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mopt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecay_steps\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstate_dict(), }\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505537227d/workspace/Documents/DeepStrain/HFpEF_ML_analysis/cnn_trials.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(data, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresults_folder, \u001b[39m'\u001b[39;49m\u001b[39mmodel-\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(stepNum) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/serialization.py:424\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/serialization.py:290\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_trn = Dataset_seg_full(patient_list.iloc[train_idx], dl_seg_path, shuffle = True, augment = False, augment_frequency = 0.5)\n",
    "dataset_val = Dataset_seg_full(patient_list.iloc[val_idx], dl_seg_path, shuffle = False, augment = False)\n",
    "model = CNNClassifier(input_channels=15)\n",
    "trainer = Trainer(model, dataset_trn, dataset_val, train_batch_size = 1, results_folder = os.path.join(save_path, 'models'), accum_iter = 1, train_num_steps = 1000, train_lr = 1e-3, train_lr_decay_every = 1000, save_models_every = 50)\n",
    "trainer.train()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x900 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot seg_ed and seg_es\n",
    "fig, ax = plt.subplots(3,5, figsize = (15,9))\n",
    "for i in range(15):\n",
    "    ax[i//5, i%5].imshow(seg_es[:,:,i])\n",
    "    ax[i//5, i%5].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
