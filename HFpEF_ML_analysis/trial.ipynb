{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import DeepStrain.functions_collection as ff\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 128):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)  # Hidden layer\n",
    "        self.output = nn.Linear(hidden_size, 1)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))  # Sigmoid activation for hidden layer\n",
    "        out = torch.sigmoid(self.output(x))  # Sigmoid activation for output layer\n",
    "        return out\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        regression_model,\n",
    "        train_num_steps,\n",
    "        save_folder,\n",
    "\n",
    "        train_lr = 1e-3,\n",
    "        train_lr_decay_every = 1,\n",
    "        save_models_every = 2,):\n",
    "        \n",
    "\n",
    "        super().__init__()\n",
    "        self.model = regression_model\n",
    "        self.input_size = regression_model.input_size\n",
    "        self.save_folder = save_folder; os.makedirs(self.save_folder, exist_ok=True)\n",
    "\n",
    "        # loss:\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = Adam(self.model.parameters(), lr = train_lr, betas = (0.9, 0.99))\n",
    "        # self.opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "        self.scheduler = StepLR(self.opt, step_size = 1, gamma=0.95)\n",
    "        self.train_lr_decay_every = train_lr_decay_every\n",
    "        self.step = 0\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.save_models_every = save_models_every\n",
    "\n",
    "    def simple_val(self,trained_model, x, y):\n",
    "        y_pred = []; y_pred_float = []\n",
    "\n",
    "        for index in range(0,x.shape[0]):\n",
    "            new_data = x[index,:]\n",
    "\n",
    "            trained_model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = trained_model(new_data); y_pred_float.append(prediction.item())\n",
    "                predicted_class = 1 if prediction.item() > 0.5 else 0; y_pred.append(predicted_class)\n",
    "\n",
    "        y_pred = np.asarray(y_pred); y_pred_float = np.asarray(y_pred_float); y_true = torch.clone(y).numpy()\n",
    "\n",
    "        # calculate accuracy using predict_collect and ground truth\n",
    "        accuracy, sensitivity, specificity,_,_,_,_ = ff.quantitative(y_pred, y_true)\n",
    "        return y_pred, y_pred_float, accuracy, sensitivity, specificity\n",
    "\n",
    "    def save_model(self, stepNum):\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.model.state_dict(),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),}\n",
    "        os.makedirs(os.path.join(self.save_folder, 'models'), exist_ok=True)\n",
    "        torch.save(data, os.path.join(self.save_folder, 'models', 'model-' + str(stepNum) + '.pt'))\n",
    "\n",
    "    def load_model(self, trained_model_filename):\n",
    "        data = torch.load(trained_model_filename)\n",
    "\n",
    "        self.model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        self.scheduler.load_state_dict(data['scheduler'])\n",
    "\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val, pre_trained_model = None ,start_step = None):\n",
    "    \n",
    "        training_log = []\n",
    "\n",
    "        # load pre-trained\n",
    "        if pre_trained_model is not None:\n",
    "            self.load_model(pre_trained_model)\n",
    "            print('model loaded from ', pre_trained_model)\n",
    "\n",
    "        if start_step is not None:\n",
    "            self.step = start_step\n",
    "\n",
    "        with tqdm(initial = self.step, total = self.train_num_steps) as pbar:\n",
    "            \n",
    "            while self.step < self.train_num_steps:\n",
    "                print('training epoch: ', self.step + 1)\n",
    "                print('learning rate: ', self.scheduler.get_last_lr()[0])\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(X_train)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(outputs.view(-1), Y_train.float())  # BCELoss expects float inputs\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "                self.step += 1\n",
    "\n",
    "                # save the model\n",
    "                if self.step !=0 and (self.step % self.save_models_every == 0):\n",
    "                   self.save_model(self.step)\n",
    "                \n",
    "                if self.step !=0 and (self.step % self.train_lr_decay_every == 0):\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                if self.step !=0 and (self.step % self.save_models_every == 0):\n",
    "                    _,_, train_accuracy, train_sensitivity, train_specificity = self.simple_val(self.model, X_train, Y_train)\n",
    "                    _,_, val_accuracy, val_sensitivity, val_specificity = self.simple_val(self.model, X_val, Y_val)\n",
    "                    print('epoch is: ', self.step, ' train loss: ', loss.item(), ' train accuracy: ', train_accuracy, ' sensitivity: ', train_sensitivity, ' specificity: ', train_specificity, ' val accuracy: ', val_accuracy, ' sensitivity: ', val_sensitivity, ' specificity: ', val_specificity) \n",
    "                            \n",
    "                    # save the training log\n",
    "                    training_log.append([self.step,self.scheduler.get_last_lr()[0], loss.item(), train_accuracy, train_sensitivity, train_specificity, val_accuracy, val_sensitivity, val_specificity])\n",
    "                    df = pd.DataFrame(training_log,columns = ['iteration','learning_rate', 'train_loss', 'train_accuracy', 'train_sensitivity', 'train_specificity', 'val_accuracy', 'val_sensitivity', 'val_specificity'])\n",
    "                    log_folder = os.path.join(self.save_folder,'log');ff.make_folder([log_folder])\n",
    "                    df.to_excel(os.path.join(log_folder, 'training_log.xlsx'),index=False)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_DataLoader():\n",
    "    def __init__(self, spreadsheet_path, data_path, index_range):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.spreadsheet = pd.read_excel(spreadsheet_path)\n",
    "        self.data_path = data_path\n",
    "        self.index_range = index_range\n",
    "\n",
    "        self.patient_list = self.spreadsheet.iloc[self.index_range]\n",
    "\n",
    "    def load_X(self):\n",
    "        X = []\n",
    "        for patient_index in range(0,self.patient_list.shape[0]):\n",
    "            patient_id_num = self.patient_list['OurID'].iloc[patient_index]\n",
    "            patient_id = ff.XX_to_ID_00XX(patient_id_num)\n",
    "\n",
    "            # load strain first:\n",
    "            strain_folder = os.path.join(self.data_path, 'results/strain', patient_id)\n",
    "            # global strain\n",
    "            global_strain = np.load(os.path.join(strain_folder, 'global_strain.npy'))\n",
    "            global_radial_strain = np.asarray(global_strain[0]).reshape(1,-1)\n",
    "            global_circumferential_strain = np.asarray(global_strain[1]).reshape(1,-1)\n",
    "\n",
    "            # layer strain\n",
    "            layer_strain = np.load(os.path.join(strain_folder, 'layer_strain.npy'))\n",
    "            layer_radial_strain = layer_strain[0,:].reshape(1,-1)\n",
    "            layer_circumferential_strain = layer_strain[1,:].reshape(1,-1)\n",
    "\n",
    "            # polar strain\n",
    "            polar_strain = np.load(os.path.join(strain_folder, 'polar_strain.npy'), allow_pickle=True)\n",
    "            Ecc_aha = np.asarray(polar_strain[5][:-1]).reshape(1,-1)\n",
    "            Err_aha = np.asarray(polar_strain[7][:-1]).reshape(1,-1)\n",
    "\n",
    "            # WTCI\n",
    "            wtci = np.load(os.path.join(strain_folder, 'wtci.npy'), allow_pickle=True)\n",
    "            wtci_aha = np.asarray(wtci[4][:-1]).reshape(1,-1)\n",
    "\n",
    "            # load geometry then:\n",
    "            geometry_folder = os.path.join(self.data_path, 'results/geometry', patient_id)\n",
    "            # circular index\n",
    "            circular_index = np.load(os.path.join(geometry_folder, 'circular_index_collect.npy')).reshape(1,-1)\n",
    "            # centers\n",
    "            centers = np.load(os.path.join(geometry_folder, 'centers_collect.npy'), allow_pickle=True)\n",
    "            center_delta_to_base = centers[1][1:,:]\n",
    "            # euclidean distance from center_delta_to_base, first row is x and second row is y\n",
    "            center_delta_to_base_euclidean = np.sqrt(np.sum(np.square(center_delta_to_base), axis=1)).reshape(1,-1)\n",
    "            # enclosed_area = np.asarray(centers[-1]).reshape(1,-1)\n",
    "\n",
    "            # axis len\n",
    "            axis = np.load(os.path.join(geometry_folder, 'axis_len_collect.npy'), allow_pickle=True)\n",
    "            major_axis = axis[:-1,0].reshape(1,-1)\n",
    "            minor_axis = axis[:-1,1].reshape(1,-1)\n",
    "\n",
    "            # concatenate all features\n",
    "            features = np.concatenate((global_radial_strain,   # 0\n",
    "                                       global_circumferential_strain,  # 1\n",
    "                                    layer_radial_strain,  #2:5\n",
    "                                    layer_circumferential_strain, #5:8\n",
    "                                    Ecc_aha, #8:24\n",
    "                                    np.concatenate((Err_aha[:,0:12] , wtci_aha[:,12:16]),axis = 1), #24:40\n",
    "\n",
    "                                    # geometry\n",
    "                                    circular_index, #40:50\n",
    "                                    center_delta_to_base_euclidean, #50:59\n",
    "                                    major_axis, #59:69\n",
    "                                    minor_axis), #69:79\n",
    "                                    axis=1)\n",
    "            X.append(features)\n",
    "        X = np.squeeze(np.asarray(X))\n",
    "        return torch.from_numpy(X).float()\n",
    "\n",
    "    def load_Y(self):\n",
    "        Y = self.spreadsheet['label_for_ML'].iloc[self.index_range]\n",
    "        return torch.from_numpy(Y.to_numpy()).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "spreadsheet_path = '/mnt/mount_zc_NAS/HFpEF/data/HFpEF_data/Patient_list/Important_HFpEF_Patient_list_unique_patient_w_readmission_finalized.xlsx' \n",
    "data_path = '/mnt/mount_zc_NAS/Deepstrain'\n",
    "index_range = range(0,50)\n",
    "dataloader = simple_DataLoader( spreadsheet_path, data_path, index_range)\n",
    "X = dataloader.load_X()\n",
    "Y = dataloader.load_Y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 79])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train idx:  [ 5  6  7 11 12  8  9 10 17 20 13 14 15 22 25 16 18 19 27 33 21 23 24 34\n",
      " 37 26 28 29 39 40 30 31 32 35 41 36 38 42 43 46 44 45 47 49 48]  val idx:  [1 2 3 0 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:00<00:25,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  1\n",
      "learning rate:  0.001\n",
      "training epoch:  2\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:00<00:19, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  3\n",
      "learning rate:  0.001\n",
      "training epoch:  4\n",
      "learning rate:  0.001\n",
      "training epoch:  5\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:00<00:17, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  6\n",
      "learning rate:  0.001\n",
      "training epoch:  7\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:00<00:18, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  8\n",
      "learning rate:  0.001\n",
      "training epoch:  9\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:01<00:21,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  10\n",
      "learning rate:  0.001\n",
      "training epoch:  11\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [00:01<00:20,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  12\n",
      "learning rate:  0.001\n",
      "training epoch:  13\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [00:01<00:20,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:  14\n",
      "learning rate:  0.001\n",
      "training epoch:  15\n",
      "learning rate:  0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression(input_size \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model,  train_num_steps \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                   save_folder \u001b[39m=\u001b[39m save_folder,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                   train_lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m,  train_lr_decay_every \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,   save_models_every \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(X_train, Y_train, X_val, Y_val, pre_trained_model \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m ,start_step \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32m/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), Y_train\u001b[39m.\u001b[39mfloat())  \u001b[39m# BCELoss expects float inputs\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f67707534227d/workspace/Documents/DeepStrain/HFpEF_analysis/trial.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main script: \n",
    "save_folder = '/mnt/mount_zc_NAS/Deepstrain/HFpEF_analysis/models/logistic_regression/'\n",
    "X_train, Y_train,  train_idx, X_val, Y_val, val_idx = ff.split_train_val(X, Y, 5, 0 , save_split_file = '/mnt/mount_zc_NAS/Deepstrain/HFpEF_analysis/models/logistic_regression/data_split.npy')\n",
    "print('train idx: ', train_idx, ' val idx: ', val_idx)\n",
    "model = LogisticRegression(input_size = X_train.shape[-1])\n",
    "trainer = Trainer(model,  train_num_steps = 200,\n",
    "                  save_folder = save_folder,\n",
    "                  train_lr = 1e-3,  train_lr_decay_every = 100,   save_models_every = 100,)\n",
    "\n",
    "trainer.train(X_train, Y_train, X_val, Y_val, pre_trained_model = None ,start_step = None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
